First I tried to modify the baseline provided for this challenges by adding more convolutional layers, 
and then i tried to fine-tune DinoV2 with different hyperParameters and different varriations .

Through the training process, I observed results for each model with below defined parameters
( epochs=10, lr=0.0001, batch size 32) 

after these parameters i observed the behaviors of model over-fitting and under fitting and based on
that i updated the hyper parameters

the first method where i try to extended the based is, A convolutional neural network with three convolutional layers (kernel sizes: 5, 5, 3), batch normalization, max pooling, an adaptive average pooling layer, 
a fully connected layer with dropout, and a final linear layer for classification.

the results were 0.00135 on test error, even worse than the baseline, i realize that extended the simple baseline would not work

so i started looking for pretarined models as per provided criteria.
i selected two  vit(vision transformer) and DinoV2(suggested one). 

i first strated with DinoV2, i decided to use the pretrained DinoV2 and fine-tune it on provided imageNet-sketch dataset.
i freeze the backbone of model and unfreze only the last classification layer( and this i use for all experimnets i did with dinov2)

for dinov2 i load models using torch

Experimnets.
I did experimnets with different version on dinov2, starting with the small model dinov2_vits14,and then some experimnets with dinov2_vitb14, dinov2_vitl14
the loss function i tried with dinov2 is crossEntropy loss with AdamW optimizer  

the different hyperParameters i tried with dinov2_vits14 are
	with dinvo v2 dinov2_vits14_reg with lr = 0.001 batch size 32, 30 epochs, accuracy on test set 0.86
    with dinvo v2 dinov2_vits14_reg lr =0.0001 batch size 32 40 epochs,, accuracy on test set 0.86
    due to overfitting dinov2_vits14_reg - batch size 64, lr=0.00141 epoch 20 weight decay 0.01,, accuracy on test set 0.86
    due to overfitting overfitting dinov2_vits14_reg - batch size 32, lr=0.00001 epoch 40 -- weight decay 0.01,, accuracy on test set 0.86
all above expeimnets with a simple data transformer dinov2_data_transforms = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
	
when trying so much and no improvemnet in accuracy i decided to try to update my datatransform # Define the transformations
dinov2_data_transforms_extended = transforms.Compose([
    # Resize to target scale (keep aspect ratio intact)
    transforms.Resize((256, 256)),
    # Randomly zoom into parts of the image
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    # Apply random rotation
    transforms.RandomRotation(degrees=15),
    # Normalize using ImageNet's mean and std values
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

but it din't improve, so i changed to dinov2_vits14_reg model, and
i did following experimnets and used the dinov2_data_transforms_extended 
  --model_name "dinov2_vitb14_reg" --batch-size 32 --lr 0.0001 --epochs 20 (undefitting so i changed learning rate to 0.001)
  ----model_name "dinov2_vitb14_reg" --batch-size 64 --lr 0.00141 --epochs 20, dinov2_data_transforms_extended
but it only improves accuracy to 1% more then previous experiments 


then the last experiment i tried with dinov2_vitl14  --batch-size 64  --lr 0.00141 --epochs 10, and i get accuracy _________.


After that i move to vision-transformations and i used google/vit-base-patch16-224 from hugging face with following paremeters

with vit lr=0.001 batch size 32 epoch 10
  with vit 64 batch size, lr=0.00141 epoch 30 
the above two experiments i did with   simple data transforms vit_data_transforms= transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
  If you scale BS by k, scale LR by the square root of k to keep variance in the gradient equal.
  https://arxiv.org/abs/1711.00489
  overfitting after 15 epochs - weight decay -> weight_decay=0.001 - with updated data transformer vit_updated_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

the accuracy i get for the test set on kaggle was lower then dinvov2 experimnets i got 78% so i didn't done much expeimnets with this